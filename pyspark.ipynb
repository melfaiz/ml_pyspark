{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Why Spark ?\r\n",
    "\r\n",
    "Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\r\n",
    "\r\n",
    "Apache Spark has an advanced DAG (Directed Acyclic Graph) execution engine that supports acyclic data flow and in-memory\r\n",
    "computing.\r\n",
    "\r\n",
    "Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including\r\n",
    "HDFS, Cassandra, HBase, and S3.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to start with Pyspark ?\r\n",
    "\r\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pyspark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "\r\n",
    "spark = SparkSession.builder.appName(\"LinearRegression\") \\\r\n",
    "                            .config(\"spark.config\", \"value\") \\\r\n",
    "                            .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\r\n",
    "    .csv(\"iris.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Core Concepts\r\n",
    "\r\n",
    "• Job: A piece of code which reads some input from HDFS or local, performs some computation on the data and writes some output data.\r\n",
    "\r\n",
    "• Stages: Jobs are divided into stages. Stages are classified as a Map or reduce stages (Its easier to understand if you have worked on Hadoop and want to correlate). Stages are divided based on computational boundaries, all computations (operators) cannot be Updated in a single Stage. It happens\r\n",
    "over many stages.\r\n",
    "\r\n",
    "• Tasks: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor (machine).\r\n",
    "\r\n",
    "• DAG: DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\r\n",
    "\r\n",
    "• Executor: The process responsible for executing a task.\r\n",
    "\r\n",
    "• Master: The machine on which the Driver program runs.\r\n",
    "\r\n",
    "• Slave: The machine on which the Executor program runs.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create RDD\r\n",
    "\r\n",
    "Usually, there are two popular ways to create the RDDs: loading an external dataset, or distributing a set of collection of objects. The following examples show some simplest ways to create RDDs by using parallelize() fucntion which takes an already existing collection in your program and pass the same to the Spark Context.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "\r\n",
    "spark = SparkSession \\\r\n",
    "        .builder \\\r\n",
    "        .appName(\"Python Spark create RDD example\") \\\r\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\r\n",
    "        .getOrCreate()\r\n",
    "\r\n",
    "Employee = spark.createDataFrame([\r\n",
    "                ('1', 'Joe', '70000', '1'),\r\n",
    "                ('2', 'Henry', '80000', '2'),\r\n",
    "                ('3', 'Sam', '60000', '2'),\r\n",
    "                ('4', 'Max', '90000', '1')],\r\n",
    "                ['Id', 'Name', 'Sallary','DepartmentId']\r\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "Employee.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sallary: string (nullable = true)\n",
      " |-- DepartmentId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spark Operations\r\n",
    "\r\n",
    "There are two main types of Spark operations: Transformations and Actions \r\n",
    "\r\n",
    "Transformations construct a new RDD from a previous one. For example, one common transformation is filtering data that matches a predicate.\r\n",
    "\r\n",
    "Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rdd.DataFrame vs pd.DataFrame\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\r\n",
    "col_name = ['A', 'B', 'C']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "pd.DataFrame(my_list,columns= col_name)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  a  1  2\n",
       "1  b  2  3\n",
       "2  c  3  4"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "spark = SparkSession \\\r\n",
    "        .builder \\\r\n",
    "        .appName(\"Python Spark create RDD example\") \\\r\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\r\n",
    "        .getOrCreate()\r\n",
    "\r\n",
    "df = spark.createDataFrame(my_list, col_name)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}