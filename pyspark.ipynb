{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Why Spark ?\r\n",
    "\r\n",
    "Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\r\n",
    "\r\n",
    "Apache Spark has an advanced DAG (Directed Acyclic Graph) execution engine that supports acyclic data flow and in-memory\r\n",
    "computing.\r\n",
    "\r\n",
    "Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including\r\n",
    "HDFS, Cassandra, HBase, and S3.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to start with Pyspark ?\r\n",
    "\r\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pyspark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "\r\n",
    "spark = SparkSession.builder.appName(\"LinearRegression\") \\\r\n",
    "                            .config(\"spark.config\", \"value\") \\\r\n",
    "                            .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\r\n",
    "    .csv(\"iris.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Core Concepts\r\n",
    "\r\n",
    "• Job: A piece of code which reads some input from HDFS or local, performs some computation on the data and writes some output data.\r\n",
    "\r\n",
    "• Stages: Jobs are divided into stages. Stages are classified as a Map or reduce stages (Its easier to understand if you have worked on Hadoop and want to correlate). Stages are divided based on computational boundaries, all computations (operators) cannot be Updated in a single Stage. It happens\r\n",
    "over many stages.\r\n",
    "\r\n",
    "• Tasks: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor (machine).\r\n",
    "\r\n",
    "• DAG: DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\r\n",
    "\r\n",
    "• Executor: The process responsible for executing a task.\r\n",
    "\r\n",
    "• Master: The machine on which the Driver program runs.\r\n",
    "\r\n",
    "• Slave: The machine on which the Executor program runs.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}